{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46bc3b7-9395-45f3-9cee-bcd2412c2d8b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052f48ba-a6a4-47f4-82d8-e7957f24c731",
   "metadata": {},
   "source": [
    "\n",
    "'''\n",
    "This code defines a process for web scraping, text chunking, vectorizing, and creating a \n",
    "retrieval-augmented generation (RAG) pipeline using Langchain and FAISS. It scrapes a webpage, \n",
    "splits the content into smaller chunks, converts these chunks into vector embeddings, stores \n",
    "them in a FAISS vector store, and then creates a chain to retrieve relevant information and \n",
    "generate responses to questions based on the retrieved context.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pip install faiss-cpu\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def crawl_data(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 200, \n",
    "        chunk_overlap = 20)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    return split_docs\n",
    "\n",
    "def vectorization(docs):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embedding)\n",
    "    return vectorStore\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    llm_model = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo')\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template('''\n",
    "            Answer the questions: \n",
    "            Context: {context}\n",
    "            Question: {input}\n",
    "            ''')  \n",
    "    \n",
    "    # Create llm chain\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm = llm_model, \n",
    "        prompt = prompt\n",
    "    )\n",
    "    \n",
    "    #A retrieval-augmented generation (RAG) pipeline, which combines a language model with a vector store \n",
    "    #to answer questions based on the context retrieved from the vector store.\n",
    "    retriever = vectorStore.as_retriever() # give 5 best results  or you can use as_retriever(search_kwargs={\"k\": 1})\n",
    "    retriever_chain = create_retrieval_chain(retriever, chain)\n",
    "\n",
    "    return retriever_chain\n",
    "    \n",
    "\n",
    "\n",
    "docs = crawl_data('https://www.cicnews.com/2024/07/express-entry-1390-candidates-invited-in-latest-pnp-only-draw-0745309.html#gs.bs9dnf')\n",
    "vectorStore = vectorization(docs)\n",
    "chain = create_chain(vectorStore)\n",
    "\n",
    "\n",
    "response = chain.invoke({\n",
    "    'input': 'What is category-based selection?',\n",
    "    'context': docs\n",
    "})\n",
    "print(response)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1742f15-5b92-4816-bcc8-3ea3159fa274",
   "metadata": {},
   "source": [
    "print(response['answer'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fa2119e5-475c-48c6-9d60-f30ca6ea2093",
   "metadata": {},
   "source": [
    "# Improve the chat using history of chats (Conversatioal ChatBot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5c320-dfb9-4f88-9f82-c85baec4f17e",
   "metadata": {},
   "source": [
    "### Showing our current approach using LangChain does not have our history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cff4d89c-f4d9-4589-9a8f-c383e859d0eb",
   "metadata": {},
   "source": [
    "# pip install faiss-cpu\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def crawl_data(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 400, \n",
    "        chunk_overlap = 20)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    return split_docs\n",
    "\n",
    "def vectorization(docs):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embedding)\n",
    "    return vectorStore\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    llm_model = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo')\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template('''\n",
    "            Answer the questions: \n",
    "            Context: {context}\n",
    "            Question: {input}\n",
    "            ''')  \n",
    "    \n",
    "    # Create llm chain\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm = llm_model, \n",
    "        prompt = prompt\n",
    "    )\n",
    "    \n",
    "    #A retrieval-augmented generation (RAG) pipeline, which combines a language model with a vector store \n",
    "    #to answer questions based on the context retrieved from the vector store.\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 3}) # give 5 best results  or you can use as_retriever(search_kwargs={\"k\": 1})\n",
    "    retriever_chain = create_retrieval_chain(retriever, chain)\n",
    "\n",
    "    return retriever_chain\n",
    "    \n",
    "\n",
    "def chat_pipe(chain, question):\n",
    "    response = chain.invoke({\n",
    "    'input': question,\n",
    "    'context': docs\n",
    "    })\n",
    "    return (response['answer'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    docs = crawl_data('https://www.cicnews.com/2024/07/express-entry-1390-candidates-invited-in-latest-pnp-only-draw-0745309.html#gs.bs9dnf')\n",
    "    vectorStore = vectorization(docs)\n",
    "    chain = create_chain(vectorStore)\n",
    "\n",
    "    while True:\n",
    "        user_input = input('You:')\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = chat_pipe(chain, user_input)\n",
    "        \n",
    "        print('Assistant:', response)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "21a4a14c-93e3-4404-a150-e7f0fcd1edaa",
   "metadata": {},
   "source": [
    "### Adding Chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8899b623-b5e6-43c2-88a8-12a20be6b994",
   "metadata": {},
   "source": [
    "# pip install faiss-cpu\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def crawl_data(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 400, \n",
    "        chunk_overlap = 20)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    return split_docs\n",
    "\n",
    "def vectorization(docs):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embedding)\n",
    "    return vectorStore\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    llm_model = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo')\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the the user question based on the context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name = 'chat_history'),\n",
    "        (\"human\", \"{input}\")   \n",
    "    ])  \n",
    "    \n",
    "    # Create llm chain\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm = llm_model, \n",
    "        prompt = prompt\n",
    "    )\n",
    "    \n",
    "    #A retrieval-augmented generation (RAG) pipeline, which combines a language model with a vector store \n",
    "    #to answer questions based on the context retrieved from the vector store.\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 3}) # give 5 best results  or you can use as_retriever(search_kwargs={\"k\": 1})\n",
    "    retriever_chain = create_retrieval_chain(retriever, chain)\n",
    "\n",
    "    return retriever_chain\n",
    "    \n",
    "\n",
    "def chat_pipe(chain, question, chat_history):\n",
    "    response = chain.invoke({\n",
    "    'input': question,\n",
    "    'chat_history': chat_history,\n",
    "    'context': docs\n",
    "    })\n",
    "    return (response['answer'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    docs = crawl_data('https://www.cicnews.com/2024/07/express-entry-1390-candidates-invited-in-latest-pnp-only-draw-0745309.html#gs.bs9dnf')\n",
    "    vectorStore = vectorization(docs)\n",
    "    chain = create_chain(vectorStore)\n",
    "\n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input('You:')\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = chat_pipe(chain, user_input, chat_history)\n",
    "\n",
    "        chat_history.append(HumanMessage(content = user_input))\n",
    "        chat_history.append(AIMessage(content = response)) \n",
    "        \n",
    "        \n",
    "        print('Assistant:', response)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2001a29c-4963-4ea4-bd98-b98f44c3360c",
   "metadata": {},
   "source": [
    "# Context Aware Retriever "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5ea0c-b8ec-4b3f-a752-d4c790d4a439",
   "metadata": {},
   "source": [
    "In a context-aware retriever, we:\n",
    "\n",
    "- **Incorporate Context**: Use the broader context, including previous queries and responses, to understand the user's intent better.\n",
    "- **Enhance Accuracy**: Improve the precision of search results by disambiguating queries and prioritizing relevant information.\n",
    "- **Leverage LLMs**: Integrate with large language models to interpret context and guide the retrieval process.\n",
    "- **Maintain Interaction History**: Keep a history of interactions to maintain continuity and coherence across multiple turns.\n",
    "\n",
    "Unlike basic retrievers that only match queries to documents, context-aware retrievers leverage the surrounding information, such as conversation history or user intent, to fetch more relevant results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c477862d-a2f2-4d1a-91ea-d12b01f52f9f",
   "metadata": {},
   "source": [
    "# pip install faiss-cpu\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder \n",
    "\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def crawl_data(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 400, \n",
    "        chunk_overlap = 20)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    return split_docs\n",
    "\n",
    "def vectorization(docs):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embedding)\n",
    "    return vectorStore\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    llm_model = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo')\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the the user question based on the context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name = 'chat_history'),\n",
    "        (\"human\", \"{input}\")   \n",
    "    ])  \n",
    "    \n",
    "    # Create llm chain\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm = llm_model, \n",
    "        prompt = prompt\n",
    "    )\n",
    "    \n",
    "    #A retrieval-augmented generation (RAG) pipeline, which combines a language model with a vector store \n",
    "    #to answer questions based on the context retrieved from the vector store.\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 3}) # give 5 best results  or you can use as_retriever(search_kwargs={\"k\": 1})\n",
    "    \n",
    "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"human\", \"Given above conversation, generate a search query to look up in order to get information relevant to the conversation.\")\n",
    "    ])\n",
    "    \n",
    "    history_aware_retriever =  create_history_aware_retriever(\n",
    "        llm = llm_model, \n",
    "        retriever = retriever,\n",
    "        prompt = retriever_prompt\n",
    "    )\n",
    "    \n",
    "    retriever_chain = create_retrieval_chain(history_aware_retriever, chain)\n",
    "\n",
    "    return retriever_chain\n",
    "    \n",
    "\n",
    "def chat_pipe(chain, question, chat_history):\n",
    "    response = chain.invoke({\n",
    "    'input': question,\n",
    "    'chat_history': chat_history,\n",
    "    'context': docs\n",
    "    })\n",
    "    return (response['answer'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    docs = crawl_data('https://www.cicnews.com/2024/07/express-entry-1390-candidates-invited-in-latest-pnp-only-draw-0745309.html#gs.bs9dnf')\n",
    "    vectorStore = vectorization(docs)\n",
    "    chain = create_chain(vectorStore)\n",
    "\n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input('You:')\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = chat_pipe(chain, user_input, chat_history)\n",
    "\n",
    "        chat_history.append(HumanMessage(content = user_input))\n",
    "        chat_history.append(AIMessage(content = response)) \n",
    "        \n",
    "        \n",
    "        print('Assistant:', response)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7399bee6-53c9-495b-aa02-c41255e42e52",
   "metadata": {},
   "source": [
    "## Create an AI agent using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eee5f-28be-46df-89f1-cd9efb4b62cf",
   "metadata": {},
   "source": [
    "### Agents: We can give them an instruction and it will determine which actions to take and in which sequence"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0525060c-8a08-43ce-8858-1f43728ba0f8",
   "metadata": {},
   "source": [
    "\n",
    "'''\n",
    "This code creates an AI agent using the GPT-3.5-turbo model, which integrates a web search tool (TavilySearchResults) \n",
    "to answer user questions based on the most current data available on the web. \n",
    "'''\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "\n",
    "#create_openai_functions_agent: create the definition of an agent\n",
    "#AgentExecutor: To invoke our agent\n",
    "#Different types of Agents in LangChain: https://python.langchain.com/v0.1/docs/modules/agents/agent_types/\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "'''\n",
    "TavilySearchResults is a tool within the LangChain ecosystem designed to integrate with \n",
    "search functionalities. It typically allows an agent to perform web searches and retrieve \n",
    "results that can be used to answer questions or provide information based on \n",
    "the most current data available on the web.\n",
    "'''\n",
    "\n",
    "model = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the the user question.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name = 'agent_scratchpad') \n",
    "    ])  \n",
    "\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "agent  = create_openai_functions_agent(\n",
    "    llm = model,\n",
    "    prompt = prompt, \n",
    "    tools =  tools)\n",
    "\n",
    "agentExecutor = AgentExecutor(\n",
    "    agent = agent, \n",
    "    tools = tools\n",
    "    )\n",
    "\n",
    "response = agentExecutor.invoke({\n",
    "    \"input\": \"What is current weather in London, Ontario, Canada\"})\n",
    "\n",
    "print(response)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e169402b-c7ef-44e1-9f99-50e9c1bd9121",
   "metadata": {},
   "source": [
    "# Agent with History (Context-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266238da-c234-41d2-922b-5755f2213b4d",
   "metadata": {},
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "#create_openai_functions_agent: create the definition of an agent\n",
    "#AgentExecutor: To invoke our agent\n",
    "#Different types of Agents in LangChain: https://python.langchain.com/v0.1/docs/modules/agents/agent_types/\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the the user question.\"),\n",
    "        MessagesPlaceholder(variable_name ='chat_history'),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name = 'agent_scratchpad') # Dynamically insert the conversation history into the prompt\n",
    "    ])  \n",
    "    \n",
    "\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "agent  = create_openai_functions_agent(\n",
    "    llm = model,\n",
    "    prompt = prompt, \n",
    "    tools =  tools)\n",
    "\n",
    "agentExecutor = AgentExecutor(\n",
    "    agent = agent, \n",
    "    tools = tools\n",
    "    )\n",
    "\n",
    "def chat_pipe(agentExecutor, user_input, chat_history):\n",
    "    response = agentExecutor.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    \n",
    "    return response['output']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input('You:')\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = chat_pipe(agentExecutor, user_input, chat_history)\n",
    "\n",
    "        chat_history.append(HumanMessage(content = user_input))\n",
    "        chat_history.append(AIMessage(content = response)) \n",
    "        \n",
    "        \n",
    "        print('Assistant:', response)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8569d8c-c7b2-4f8d-9d70-6021f386c4c5",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a9ccb-e725-4a30-92a6-e5b8e3851e44",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cb4e8-c28f-43b8-9915-488c4120cbfe",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec87486-eb74-4db6-8b5d-7f91f8453a1d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19567fa2-b384-48bc-8d6d-4a541561c3c8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5edd9-1ede-4794-8af3-c216a01b6244",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78970f8-fda3-45e4-ac0d-50d5695c5621",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47569515-8255-4ba5-a62a-1872bf63d752",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cd25f-c6ac-4e9f-8bb9-15c10a7b310a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2335c5-5b68-4c7d-b04c-63a1ce02e0df",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638cfaa-35a4-4628-bf07-b1aa984048cd",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6720f-8559-488c-b51a-d45fba625300",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908f7c7-60a6-4096-9619-0c4831ab3c1f",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
